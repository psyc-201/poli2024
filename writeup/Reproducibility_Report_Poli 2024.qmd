---
title: "Reproducibility Report for Poli et al. (2024, Developmental Science)"
author: "Victoria Hennessy (vhennessy@ucsd.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Reproducibility reports should all use this template to standardize reporting across projects. These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

*\[No abstract is needed.\] Each reproducibility project will have a straightforward, no frills report of the study and reproducibility results. These reports will be publicly available as supplementary material for the aggregate report(s) of the project as a whole. Also, to maximize project integrity, the intro and methods will be written and critiqued in advance of your attempt to reproduce the results. Introductions can be just 1-2 paragraphs clarifying the main idea of the original study, the target finding for your reproducibility attempt, and any other essential information. It will NOT have a literature review -- that is in the original publication. You can write both the introduction and the methods in past tense.*

**Clarify key analysis of interest here: Hierarchical Bayesian model to reproduce individual estimates of infant curiosity from their looking behavior**

### Justification for choice of study

My current line of research focuses on individual differences in early information seeking. To study infant learning strategies, Poli and colleagues (2020; 2024) have developed an analytic pipeline that leverages an infant-friendly visual learning (VL) task from which information-theoretic measures (e.g., **predictability and information gain**) can be derived for each trial in sequences of visual events.

These measures are then incorporated into a hierarchical Bayesian model fitted to real infants' looking behavior (look-aways, looking time, and saccadic latency) in the VL task to infer the values of latent parameters that represent specific cognitive functions: **processing speed**, **learning performance**, **curiosity**, and **sustained attention** (see Figure 1).

[![Figure 1. The research pipeline in Poli et al. 2024 (Figure 2 in the original paper). The current project will focus on the first three boxes outlined in yellow: (1) reproducing the information-theoretic measures from the stimuli in the visual learning task, (2) simulating infant eye-tracking data during the VL task, and (3) estimating the latent parameters at the individual level.](images/poli2024researchpipeline-01.jpg)](https://doi.org/10.1111/desc.13460)

For the proposed project, I have three primary aims:

**(1)** Reproduce the findings using the original data and code.

**(2)** Simulate infant looking behavior in the visual learning task, e.g., looking time in each trial.

**(3)** Fit a hierarchical Bayesian model to estimate latent parameter(s) of interest using the simulated data, e.g., learning performance (saccadic latency x predictability) and/or curiosity (stimulus informativity x looking time).

**(4)** Compare model fit and analyses from simulated data to the original data (Poli et al., 2024).

***Working through the described pipeline will help me develop the analytic toolkit and computational understanding needed for my own research program.***

### Anticipated challenges

Simulating this data and analytic pipeline involves three high-level stages: input generation, hierarchical modeling, and outcome replication, each of which will pose their own computational challenges.

The code to generate the sequences for the visual learning task is publicly available. I will follow Poli et al. (2020 and 2024) to attempt to derive the information-theoretic measures for each trial in the visual learning task. This may be complex and time-consuming to undertake from scratch given the scope of the current project. In the event that this step hinders the completion of the subsequent step, I will reference the original analysis script, found [here](https://data.ru.nl/collections/di/dcc/DSC_2019.00056_127), and/or simulate just a portion of the looking behavior (e.g., looking time) to simplify the process.

### Links

[Project Repository](https://github.com/thenness-y/poli2024)

[Original Paper](https://github.com/thenness-y/poli2024/blob/main/original_paper/Poli2023DevSci.pdf)

[Original Data Repository](https://osf.io/zux9v/overview)

## Methods

### **Reproduction:**

#### Data Preparation

1.  Load the raw data for the visual learning task.
    -   Combine the two datasets, `Roris_nostd.csv` and `Roris_smiley.csv`.
2.  Convert to model-friendly format.
3.  Z-score the dependent variables (looking time and saccadic latency)
4.  Z-score the independent variables (information gain, predictability, and surprise)
5.  Handle missing values.

Verify the following columns are present:

-   `subj`: subject ID
-   `nseq`: sequence number
-   `ntrialseq`: trial number within sequence
-   `dwell`: looking time to sequence
-   `slat`: saccadic latency
-   `event`: look-away (binary, either 0 or 1)
-   **`D`: KL divergence or *information gain*** (pre-computed)
-   **`H`: entropy or *predictability*** (pre-computed)
-   `I`: ***surprise*** (pre-computed)

#### Analysis Pipeline

1.  [x] **Fit the hierarchical Bayesian model (HBM)**

    -   Specify relationships of interest (i.e., between looking behavior and latent parameters).
    -   Fit the model using ADVI sampling for testing, MCMC for final reproduction.
    -   Derive individual posterior distributions for:
        -   **Learning Performance**: β₁\^SL (correlation between saccadic latency and predictability)
        -   **Curiosity**: β₁\^LT (correlation between looking time and information gain)

2.  [ ] **Model validation**

    -   Check convergence.

        R\^ threshold should be \< 1.004 if using MCMC. Qualitative check with ADVI.

    -   (If MCMC) Compare model fit between hierarchical vs. simple (group-only) model

        *Necessary to confirm the model is working correctly. If we don't find that infants look longer at more informative stimuli on average (group infant), model fitting may be wrong. Group-level results should match original findings before proceeding. Must verify that the hierarchical model fits better to prove that individual differences exist.*

        *Authors compared models using WAIC and LOO. Full model (includes individual differences) should have lower scores, indicating a* *better fit. Here's the code chunk where they did this:*

        ``` python
        model_waic1 = az.loo(idata, var_name="LT_like")
        model_waic2 = az.waic(idata, var_name="LT_like")
        ```

3.  [x] **Extract individual differences**

    -   Once model has ran, compute posterior distributions for each parameter.

        *The HBM produces a probability distribution for each parameter for each infant (e.g., infant X's "learning performance" isn't a single number, but a distribution of plausible values within a CI with 89% confidence). The model represents this as 20,000 samples from the distribution.*

    ``` python
    posterior=pd.DataFrame()
    posterior["subjnum"]= markasgood.values.reshape(-1, )
    posterior["LT0"]=np.median(trace["LT0"], axis=0)  # Processing speed (looking time)
    posterior["LT1"]=np.median(trace["LT1"], axis=0)  # Curiosity
    posterior["SL0"]=np.median(trace["SL0"], axis=0)  # Processing speed (saccadic)
    posterior["SL1"]=np.median(trace["SL1"], axis=0)  # Learning performance
    posterior["lambda0"]=np.median(trace["lambda0"], axis=0)  # Sustained attention
    posterior["beta_LA"]=np.median(trace["beta_LA"], axis=0)  # (not used in main analysis)
    posterior.to_csv('posterior_median.csv')
    ```

    -   Save to an output file in which there is one row per infant; columns = latent parameters.

    -   Identify infants with significant individual effects (89% credible intervals excludes zero)

### Simulation:

\*Indicates steps that may require assistance from the instructional team.

1.  Define the individual sequence and trial structure using `sequences_t.csv`, available [here](https://osf.io/a93qr/files), which contains the target locations.

2.  \*Use the formulas from the supplementary materials to compute the information-theoretic variables for each sequence (or request this code from Poli and colleagues).

3.  \*Generate individual parameters for 106 infants (e.g., baseline looking time, processing speed, etc) .

4.  \*Generate behavioral data for each infant (e.g., looking time, saccadic latency, etc).

5.  Preprocess using same steps as Reproduction.

6.  Test model recovery:

    **Specifically, I aim to recover similar posterior distributions for at least one of the latent parameters (e.g., curiosity) and achieve a comparable model fit with the simulated data.**

### Differences from original study

The computing environments are the same. Visualizations may be carried out in R, but the model and any pre-processing steps will be in Python.

## Project Progress Check 1

### Measure of success

The outcome measure will be a successful reproduction of their findings based on the latent parameters from the Bayesian model. I aim to reproduce the following as indexed by 89% credible intervals different from zero (where zero indicates lack of an effect) with both the original and the simulated data:

-   **Learning Performance (saccadic latency x predictability)**: approximately 57 infants (40%) with a significant coefficient.

-   **Curiosity (looking time x information gain)**: approximately 31 infants (22%) with a significant coefficient

### Pipeline progress

1.  Modernized the original modeling script:
    -   Updated pymc3 to pymc, theano to pytensor
    -   Removed masked arrays since PyMC handles missing values automatically
2.  Changed sampling for testing purposes:
    1.  Switched to ADVI

        -   Original paper and findings use MCMC with 500K tuning + 10K sampling (I have not have the time or computational capacity to run this yet).

    2.  Added convergence tracking, basically records the mean and SD of ADVI's approximation at every iteration and allows it to improve its Gaussian approximation to the posterior; the tracking lets me see if it's converging.

    3.  Commented out WAIC/LOO, for now. Trying to run these w/ ADVI's approximation + severe PyTensor C++ issues caused big issues that were beyond my understanding...

    4.  In the meantime, I just checked my ADVI convergence. The lines flattened out by iteration 10k, indicating that the model successfully reached an "optimal" approximation of the posteriors:

        ![](images/advi_convergence-01.png)
3.  Fit the model w/ ADVI sampling:

-   I did this several times with data from just 3 subjects for testing purposes, and then with all subjects in the combined dataset.

4.  Ran a basic analysis script (advi_analysis.py) to evaluate the significance of individual subject coefficients from the model based on 89% credible intervals w/out zero.

    Preliminary Results w/ ADVI Sampling:

    **Learning Performance (saccadic latency x predictability)**: approximately 63 infants (43.45%) with a significant coefficient (SL1).

    **Curiosity (looking time x information gain)**: approximately 40 infants (27.59%) with a significant coefficient (LT1).

In summary, I completed the steps in the Reproduction section of my Methods, *kind of*. There are still major issues related to the model sampling strategy that I need to resolve.

## Results

### Data preparation

Data preparation following the analysis plan.

```{r include=F}
### Data Preparation

#### Load Relevant Libraries and Functions

#### Import data

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
```

### Key analysis

The analyses as specified in the analysis plan.

*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).

## Discussion

### Summary of Reproduction Attempt

Open the discussion section with a paragraph summarizing the primary result from the key analysis and assess whether you successfully reproduced it, partially reproduced it, or failed to reproduce it.

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis of the dataset, (b) assessment of the meaning of the successful or unsuccessful reproducibility attempt - e.g., for a failure to reproduce the original findings, are the differences between original and present analyses ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the reproducibility attempt (if you contacted them). None of these need to be long.
